{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc54ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "from tensorflow import keras as k\n",
    "from tensorflow.keras.layers import *\n",
    "from funciones import *\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "tamaño=450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342c88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "from tensorflow.keras.layers import *\n",
    "from funciones import *\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea9f635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodulos=pd.read_csv(\"nodulos_descripcion.csv\",header=0,index_col=0)\n",
    "df_nodulos.columns=[\"Forma\",\"Margen\",\"Orientación\",\"Ecogenicidad\",\"Característica Posterior\",\"Halo Ecogénico\", 'Sugestivo', 'Forma2',\n",
    "       'Margen2', 'Orientación2', 'Ecogenicidad2', 'Característica Posterior2',\n",
    "       'Halo Ecogénico2', 'Sugestivo2', 'BIRADS', 'Resultados', 'Num Nódulos']\n",
    "\n",
    "#Creamos el vocabulario que compone nuestra lista de descriptores que obtenemos de entre las opciones de la plantilla excel\n",
    "Formas=[l for l in list(set(list(df_nodulos[\"Forma\"])+list(df_nodulos[\"Forma2\"]))) if l==l]\n",
    "Margenes=[l for l in list(set(list(df_nodulos[\"Margen\"])+list(df_nodulos[\"Margen2\"]))) if l==l]\n",
    "Orientaciones=[\"paralela\",\"antiparalela\"]\n",
    "Ecogenicidades=[l for l in list(set(list(df_nodulos[\"Ecogenicidad\"])+list(df_nodulos[\"Ecogenicidad2\"]))) if l==l]\n",
    "Posteriores=[\"sin cambios\",\"refuerzo\",\"sombra\",\"mixto\"]\n",
    "Halos=[\"sí\",\"no\"]\n",
    "Sugestividades=[l for l in list(set(list(df_nodulos[\"Sugestivo\"])+list(df_nodulos[\"Sugestivo2\"]))) if l==l]\n",
    "BIRADS=[\"2\",\"3\",\"4A\",\"4B\",\"4C\",\"5\"]\n",
    "Resultados=[\"benign\",\"malignant\"]\n",
    "#Tipo=[\"quiste\",\"fibroadenoma\",\"carcinoma ductal invasivo\",\"carcinoma ductal in situ\",\"linfoma\"]\n",
    "#voc=set(Formas+Margenes+Orientaciones+Ecogenicidades+Posteriores+Halos+Sugestividades+BIRADS+Resultados+Tipo)\n",
    "voc=set(Formas+Margenes+Orientaciones+Ecogenicidades+Posteriores+Halos+Sugestividades+BIRADS+Resultados)\n",
    "voc.add(\"\\t\")\n",
    "voc.add(\"\\n\")\n",
    "voc=list(voc)\n",
    "voc.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcfbb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = dict((word, idx) for idx, word in enumerate(voc))\n",
    "idx_to_word = dict((idx, word) for idx, word in enumerate(voc))\n",
    "descripciones_x={}\n",
    "descripciones_y={}\n",
    "\n",
    "#Vamos a ver escribir las características en una sola lista por cada nódulo eliminando los valores nan.\n",
    "nodulos_x=[]\n",
    "nodulos_y=[]\n",
    "\n",
    "for ind, row in df_nodulos.iterrows():\n",
    "    nodulo_x=[\"\\t\"]\n",
    "    nodulo_y=[]\n",
    "    for carac in row[:7]:\n",
    "        if pd.isna(carac):\n",
    "            continue\n",
    "        else:\n",
    "            nodulo_x.append(str(carac))\n",
    "            nodulo_y.append(str(carac))\n",
    "    for carac in row[14:16]:\n",
    "        if pd.isna(carac):\n",
    "            continue\n",
    "        else:\n",
    "            nodulo_x.append(str(carac))\n",
    "            nodulo_y.append(str(carac))\n",
    "    nodulo_y.append(\"\\n\")\n",
    "    nodulos_x.append(nodulo_x)\n",
    "    nodulos_y.append(nodulo_y)\n",
    "    \n",
    "    if row[\"Num Nódulos\"]==2:\n",
    "        nodulo_x=[\"\\t\"]\n",
    "        nodulo_y=[]\n",
    "        for carac in row[7:16]:\n",
    "            if pd.isna(carac):\n",
    "                continue\n",
    "            else:\n",
    "                nodulo_x.append(str(carac))\n",
    "                nodulo_y.append(str(carac))\n",
    "        nodulo_y.append(\"\\n\")\n",
    "        nodulos_x.append(nodulo_x)\n",
    "        nodulos_y.append(nodulo_y)\n",
    "    \n",
    "    \n",
    "max_len_x=max([len(nodulo) for nodulo in nodulos_x])\n",
    "i=0\n",
    "for ind, row in df_nodulos.iterrows():\n",
    "    input_data = np.zeros((max_len_x, len(voc)), dtype='float32')\n",
    "    for j in range(len(nodulos_x[i])):\n",
    "        input_data[j,word_to_idx[nodulos_x[i][j]]]=1\n",
    "    descripciones_x[ind]=[input_data]\n",
    "    if row[\"Num Nódulos\"]==2:\n",
    "        input_data = np.zeros((max_len_x, len(voc)), dtype='float32')\n",
    "        i=i+1\n",
    "        for j in range(len(nodulos_x[i])):\n",
    "            input_data[j,word_to_idx[nodulos_x[i][j]]]=1\n",
    "        descripciones_x[ind].append(input_data)\n",
    "    i=i+1\n",
    "        \n",
    "i=0\n",
    "max_len_y=max([len(nodulo) for nodulo in nodulos_y])\n",
    "for ind, row in df_nodulos.iterrows():\n",
    "    input_data = np.zeros((max_len_y, len(voc)), dtype='float32')\n",
    "    for j in range(len(nodulos_y[i])):\n",
    "        input_data[j,word_to_idx[nodulos_y[i][j]]]=1\n",
    "    descripciones_y[ind]=[input_data]\n",
    "    if row[\"Num Nódulos\"]==2:\n",
    "        input_data = np.zeros((max_len_y, len(voc)), dtype='float32')\n",
    "        i=i+1\n",
    "        for j in range(len(nodulos_y[i])):\n",
    "            input_data[j,word_to_idx[nodulos_y[i][j]]]=1\n",
    "        descripciones_y[ind].append(input_data)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6fcb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solo si no se entrena antes\n",
    "EncoderLSTM = k.models.load_model('EncoderYOLOLSTM.h5')\n",
    "decoder=LSTM_Attention()\n",
    "word_embedding=Dense(len(voc),activation=\"softmax\")\n",
    "\n",
    "\n",
    "encoder_model_inf = k.Model(inputs=EncoderLSTM.inputs, outputs=EncoderLSTM.outputs)\n",
    "\n",
    "# Create decoder input states for inference\n",
    "decoder_state_input_h = Input(shape=(256))\n",
    "decoder_state_input_c = Input(shape=(256))\n",
    "features=Input(shape=(196,512))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_inf_input=Input(shape=(1,len(voc)))\n",
    "decoder_h, decoder_c, alfa = decoder([features,decoder_inf_input,decoder_input_states])\n",
    "decoder_states = [decoder_h , decoder_c, alfa]\n",
    "decoder_out = word_embedding(decoder_h)\n",
    "\n",
    "# Create decoder inference model\n",
    "decoder_model_inf = k.Model(inputs=[features,decoder_inf_input] + decoder_input_states, outputs=[decoder_out] + decoder_states )\n",
    "\n",
    "with open(\"YOLO_LSTM_weights.pkl\", \"rb\") as open_file:\n",
    "    LSTM_list = pickle.load(open_file)\n",
    "with open(\"YOLO_word_embedding_weights.pkl\",\"rb\") as open_file:\n",
    "    embedding_list=pickle.load(open_file)\n",
    "    \n",
    "decoder.set_weights(LSTM_list)\n",
    "word_embedding.set_weights(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(inputs):\n",
    "    feature = encoder_model_inf.predict(inputs)\n",
    "    target_seq = np.zeros((1,1, len(voc)))\n",
    "    target_seq[0,0, word_to_idx['\\t']] = 1\n",
    "    states_val=[np.zeros((1,256)),np.zeros((1,256))]\n",
    "    decoder_out, decoder_h, decoder_c, _ = decoder_model_inf.predict(x=[feature,target_seq]+states_val)\n",
    "    # Find out the next character from the Decoder output\n",
    "    max_val_index = np.argmax(decoder_out[0,:])\n",
    "    sampled_suffix_word = idx_to_word[max_val_index]\n",
    "    words=[]\n",
    "    words.append(sampled_suffix_word)\n",
    "    i=0\n",
    "    while (sampled_suffix_word!=\"\\n\" and i<30):\n",
    "        target_seq = np.zeros((1,1,  len(voc)))\n",
    "        target_seq[0,0, max_val_index] = 1\n",
    "        \n",
    "        # Initialize the decoder state to the states from last iteration\n",
    "        states_val = [decoder_h[0], decoder_c[0]]\n",
    "\n",
    "        # Get decoder output\n",
    "        decoder_out, decoder_h, decoder_c, _ = decoder_model_inf.predict(x=[feature,target_seq] + states_val)\n",
    "        # Get most probable next character and print it.\n",
    "        max_val_index = np.argmax(decoder_out[0,:])\n",
    "        sampled_suffix_word = idx_to_word[max_val_index]\n",
    "        words.append(sampled_suffix_word)\n",
    "        i+=1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12377648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(inputs):\n",
    "    words=generate_words(inputs)\n",
    "    words=list(dict.fromkeys(words))\n",
    "    len_frase=len(words)\n",
    "    boolean=False\n",
    "    boolean2=False\n",
    "    i_sug=0\n",
    "    caption=\"\"\n",
    "    for i,word in enumerate(words):\n",
    "        frase=\"\"\n",
    "        if word==\"/n\":\n",
    "            continue\n",
    "        elif word in Formas:\n",
    "            if word!=\"irregular\":\n",
    "                frase=word[:-1]+\"o\"\n",
    "            else:\n",
    "                frase+=word\n",
    "        elif word in Margenes:\n",
    "            frase=word\n",
    "        elif word in Orientaciones:\n",
    "            frase=\"con orientación \"+word\n",
    "        elif word in Ecogenicidades:\n",
    "            if word==\"compleja\":\n",
    "                frase=\"con ecogenicidad compleja\"\n",
    "            elif word==\"mixta\":\n",
    "                frase=\"con ecogenicidad mixta\"\n",
    "            else:\n",
    "                frase=word[:-1]+\"o\"\n",
    "        elif word in Posteriores:\n",
    "            if word==\"mixto\":\n",
    "                frase=\"con característica posterior mixta\"\n",
    "            elif word==\"sin cambios\":\n",
    "                frase=\"sin cambios posteriores\"\n",
    "            else:\n",
    "                frase=\"con \"+word+ \" posterior\"\n",
    "        elif word in Halos:\n",
    "            if word==\"no\":\n",
    "                frase=\"sin halo ecogénico\"\n",
    "            elif word==\"sí\":\n",
    "                frase=\"con halo ecogénico\"\n",
    "        elif word in Sugestividades:\n",
    "            frase=\"sugestivo de \"+ word\n",
    "            boolean=True\n",
    "            i_sug=i\n",
    "        elif word in BIRADS:\n",
    "            if boolean:\n",
    "                frase=\" (BIRADS®\"+word+\")\"\n",
    "            else:\n",
    "                i_sug=i\n",
    "                if word==\"2\":\n",
    "                    frase=\" (BIRADS® \"+word+\")\"\n",
    "                if word==\"3\":\n",
    "                    frase=\"probablemente benigno\"+\"(BIRADS® \"+word+\")\"\n",
    "                elif word==\"4A\":\n",
    "                    frase=\"de baja sospecha\"+\"(BIRADS® \"+word+\")\"\n",
    "                elif word==\"4B\":\n",
    "                    frase=\"de sospecha intermedia\"+\"(BIRADS® \"+word+\")\"\n",
    "                elif word==\"4C\":\n",
    "                    frase=\"de alta sospecha\"+\"(BIRADS® \"+word+\")\"\n",
    "                elif word==\"5\":\n",
    "                    frase=\"sugestivo de carcinoma\"+\"(BIRADS® \"+word+\")\"\n",
    "        else:\n",
    "            if not boolean2:\n",
    "                frase=\". Resultado final aprendido por base de datos: \"\n",
    "                boolean2=True\n",
    "            if word in Resultados:\n",
    "                if word==\"benign\":\n",
    "                    frase+=\"benigno\"\n",
    "                else:\n",
    "                    frase+=\"maligno\"\n",
    "#            elif word in Tipo:\n",
    "#                frase+=word\n",
    "        if frase!=\"\":\n",
    "            if i==0:\n",
    "                caption+=\"Nódulo \"+frase\n",
    "            elif word in Ecogenicidades:\n",
    "                if word[:2]==\"hi\" or word[0]==\"i\":\n",
    "                    caption+=\" e \"+frase\n",
    "                else:\n",
    "                    caption+=\" y \"+frase\n",
    "            elif i==0:\n",
    "                caption+=\"Nódulo \"+frase\n",
    "            elif frase[0]==\".\" or frase[:2]==\" (\":\n",
    "                caption+=frase\n",
    "            elif caption[-2:]==\": \":\n",
    "                caption+=frase\n",
    "            elif i==len(frase)-1:\n",
    "                caption+=\", \"+frase+\".\"\n",
    "            else:\n",
    "                caption+=\", \" +frase\n",
    "    return caption\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "969aa384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(imagen,imagen_dir=\"..\\YOLO_para_describir\\images\"):\n",
    "    imag_dir=os.path.join(imagen_dir,imagen)\n",
    "    \n",
    "    if not glob.glob(os.path.join(\"runs/detect/Yolo_test_lstm\",imagen)):\n",
    "        !python detect.py --weights YOLO_Caption448.pt --img 896 --conf 0.25 --source {imag_dir} --data data/YOLO.yaml --exist-ok --name Yolo_test_lstm --save-crop\n",
    "    original=load_img(os.path.join(\"runs/detect/Yolo_test_lstm\",imagen),color_mode=\"grayscale\")\n",
    "    \n",
    "    nodulos=glob.glob(os.path.join(\"runs/detect/Yolo_test_lstm/crops/nodulo\",imagen[:-4]+\"*\"))\n",
    "    print(\"Número de nódulos: \",len(nodulos))\n",
    "    display(original)\n",
    "    for ima in nodulos:\n",
    "        imag_or=load_img(ima,color_mode=\"grayscale\")\n",
    "        ancho,largo=imag_or.size\n",
    "        imag=img_to_array(imag_or)/255.\n",
    "        imag=tf.image.pad_to_bounding_box(\n",
    "        imag, (tamaño-largo)//2,(tamaño-ancho)//2, 450, 450\n",
    "        )\n",
    "        imag=img_to_array(imag)\n",
    "        words=generate_caption(imag.reshape(1,450,450,1))\n",
    "#        visualize(imagen=imag,alfas=np.array(alfas[:,0]))\n",
    "        display(imag_or)\n",
    "        print(words)\n",
    "    if not nodulos:\n",
    "        print(\"Sin hallazgos. (BIRADS® 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f6caaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.redirect_stdout(None):\n",
    "    %cd yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46e548",
   "metadata": {},
   "source": [
    "# Esto es una prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f98237b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e15c671caa24a59959a8a70c1af251e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Selecciona una ecografía'), FileUpload(value={}, description='Upload'), Button(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "btn_upload = widgets.FileUpload()\n",
    "out_pl=widgets.Output()\n",
    "btn_run = widgets.Button(description='Descripción')\n",
    "lbl_pred = widgets.Label()\n",
    "def on_click_descripcion(change):\n",
    "    img=Image.open(io.BytesIO(btn_upload.data[-1]))\n",
    "    name=list(btn_upload.value.keys())[-1]\n",
    "    img_dir=os.path.join(\"../Imagenes_detect\",name)\n",
    "    img.save(img_dir)\n",
    "    out_pl.clear_output()\n",
    "    with out_pl: \n",
    "        display(img)\n",
    "        words=results(name,\"../Imagenes_detect\")\n",
    "btn_run.on_click(on_click_descripcion)\n",
    "VBox([widgets.Label('Selecciona una ecografía'), \n",
    "      btn_upload, btn_run, out_pl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02570f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.7.11\n",
      "IPython version      : 7.31.1\n",
      "\n",
      "wget         : not installed\n",
      "pandas       : 1.3.5\n",
      "numpy        : 1.21.5\n",
      "geopy        : not installed\n",
      "altair       : not installed\n",
      "vega         : not installed\n",
      "vega_datasets: not installed\n",
      "watermark    : 2.3.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      " \n",
      "Last updated: Tue Mar 29 2022 18:16:18Hora de verano romance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "# python, ipython, packages, and machine characteristics\n",
    "%watermark -v -m -p wget,pandas,numpy,geopy,altair,vega,vega_datasets,watermark \n",
    "\n",
    "# date\n",
    "print (\" \")\n",
    "%watermark -u -n -t -z "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO_tf",
   "language": "python",
   "name": "yolo_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
